{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed962793-68b5-4822-a97b-fea471e696d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13cc5060-c4fd-4c9c-a9d1-c7decf54e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "107e01e8-ab3e-4020-bf73-bf917e462c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c751f21-aed4-421e-811d-d0a595e38023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\hayat\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "   ---------------------------------------- 0.0/11.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.2 MB 8.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.1/11.2 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.4/11.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.7/11.2 MB 6.2 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.2 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.2 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.2/11.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.2 MB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.2/11.2 MB 5.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "   ---------------------------------------- 0.0/46.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 1.0/46.2 MB 6.3 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 2.4/46.2 MB 5.8 MB/s eta 0:00:08\n",
      "   --- ------------------------------------ 3.7/46.2 MB 6.1 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 5.0/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 6.3/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 7.6/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 8.7/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 10.0/46.2 MB 5.9 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 11.3/46.2 MB 5.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 12.3/46.2 MB 5.8 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 13.4/46.2 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 14.7/46.2 MB 5.8 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 15.7/46.2 MB 5.8 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 17.0/46.2 MB 5.8 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 18.4/46.2 MB 5.8 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 19.4/46.2 MB 5.8 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 20.4/46.2 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 21.5/46.2 MB 5.6 MB/s eta 0:00:05\n",
      "   ------------------- -------------------- 22.5/46.2 MB 5.6 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 23.6/46.2 MB 5.6 MB/s eta 0:00:05\n",
      "   --------------------- ------------------ 24.4/46.2 MB 5.5 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 25.7/46.2 MB 5.5 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.7/46.2 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 27.8/46.2 MB 5.5 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 28.8/46.2 MB 5.5 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 30.1/46.2 MB 5.5 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 30.9/46.2 MB 5.5 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 32.2/46.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 33.6/46.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 34.6/46.2 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 35.7/46.2 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 36.7/46.2 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 38.0/46.2 MB 5.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 38.8/46.2 MB 5.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 40.4/46.2 MB 5.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 41.4/46.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 42.5/46.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 43.8/46.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  45.1/46.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  46.1/46.2 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 46.2/46.2 MB 5.4 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.13.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f473f73-84b9-48d1-8647-c72550afbc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4265f18c-946f-4c9a-8d67-55c4708c26b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Show Id                          Title  \\\n",
      "0  cc1b6ed9-cf9e-4057-8303-34577fb54477                       (Un)Well   \n",
      "1  e2ef4e91-fb25-42ab-b485-be8e3b23dedb                         #Alive   \n",
      "2  b01b73b7-81f6-47a7-86d8-acb63080d525  #AnneFrank - Parallel Stories   \n",
      "3  b6611af0-f53c-4a08-9ffa-9716dc57eb9c                       #blackAF   \n",
      "4  7f2d4170-bab8-4d75-adc2-197f7124c070               #cats_the_mewvie   \n",
      "\n",
      "                                         Description  \\\n",
      "0  This docuseries takes a deep dive into the luc...   \n",
      "1  As a grisly virus rampages a city, a lone man ...   \n",
      "2  Through her diary, Anne Frank's story is retol...   \n",
      "3  Kenya Barris and his family navigate relations...   \n",
      "4  This pawesome documentary explores how our fel...   \n",
      "\n",
      "                      Director  \\\n",
      "0                          NaN   \n",
      "1                       Cho Il   \n",
      "2  Sabina Fedeli, Anna Migotto   \n",
      "3                          NaN   \n",
      "4             Michael Margolis   \n",
      "\n",
      "                                           Genres  \\\n",
      "0                                      Reality TV   \n",
      "1  Horror Movies, International Movies, Thrillers   \n",
      "2             Documentaries, International Movies   \n",
      "3                                     TV Comedies   \n",
      "4             Documentaries, International Movies   \n",
      "\n",
      "                                                Cast Production Country  \\\n",
      "0                                                NaN      United States   \n",
      "1                           Yoo Ah-in, Park Shin-hye        South Korea   \n",
      "2                        Helen Mirren, Gengher Gatti              Italy   \n",
      "3  Kenya Barris, Rashida Jones, Iman Benson, Genn...      United States   \n",
      "4                                                NaN             Canada   \n",
      "\n",
      "   Release Date Rating  Duration Imdb Score Content Type         Date Added  \n",
      "0        2020.0  TV-MA  1 Season     6.6/10      TV Show                NaN  \n",
      "1        2020.0  TV-MA    99 min     6.2/10        Movie  September 8, 2020  \n",
      "2        2019.0  TV-14    95 min     6.4/10        Movie       July 1, 2020  \n",
      "3        2020.0  TV-MA  1 Season     6.6/10      TV Show                NaN  \n",
      "4        2020.0  TV-14    90 min     5.1/10        Movie   February 5, 2020  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"netflixData.csv\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff1a6c7c-0526-4384-ac80-1db84f40a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show Id                  0\n",
      "Title                    0\n",
      "Description              0\n",
      "Director              2064\n",
      "Genres                   0\n",
      "Cast                   530\n",
      "Production Country     559\n",
      "Release Date             3\n",
      "Rating                   4\n",
      "Duration                 3\n",
      "Imdb Score             608\n",
      "Content Type             0\n",
      "Date Added            1335\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28c1e2fa-ed4f-432d-a6f6-0be4e0174696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Title  \\\n",
      "0                       (Un)Well   \n",
      "1                         #Alive   \n",
      "2  #AnneFrank - Parallel Stories   \n",
      "3                       #blackAF   \n",
      "4               #cats_the_mewvie   \n",
      "\n",
      "                                         Description Content Type  \\\n",
      "0  This docuseries takes a deep dive into the luc...      TV Show   \n",
      "1  As a grisly virus rampages a city, a lone man ...        Movie   \n",
      "2  Through her diary, Anne Frank's story is retol...        Movie   \n",
      "3  Kenya Barris and his family navigate relations...      TV Show   \n",
      "4  This pawesome documentary explores how our fel...        Movie   \n",
      "\n",
      "                                           Genres  \n",
      "0                                      Reality TV  \n",
      "1  Horror Movies, International Movies, Thrillers  \n",
      "2             Documentaries, International Movies  \n",
      "3                                     TV Comedies  \n",
      "4             Documentaries, International Movies  \n"
     ]
    }
   ],
   "source": [
    "data = data[[\"Title\", \"Description\", \"Content Type\", \"Genres\"]]\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8490f61e-a88a-48a1-b4f1-47bd88f4be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e3bb319-f3e0-4232-85a7-e9425bc761a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hayat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopword=set(stopwords.words('english'))\n",
    "\n",
    "def clean(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = [word for word in text.split(' ') if word not in stopword]\n",
    "    text=\" \".join(text)\n",
    "    text = [stemmer.stem(word) for word in text.split(' ')]\n",
    "    text=\" \".join(text)\n",
    "    return text\n",
    "data[\"Title\"] = data[\"Title\"].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08f6336b-86ee-4589-ad7f-a8f346802d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5892                          yank\n",
      "4738                          crew\n",
      "1595                         fatso\n",
      "1756                          game\n",
      "4483    super monster furev friend\n",
      "4522                   sword trust\n",
      "3355    naruto shippûden movi bond\n",
      "528                   banana split\n",
      "1570                       fanatyk\n",
      "5829                fool fall love\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.Title.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "252e7f95-c7ad-49c1-847e-4ac35c2d895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "feature = data[\"Genres\"].tolist()\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\") \n",
    "tfidf_matrix = tfidf.fit_transform(feature)\n",
    "similarity = cosine_similarity(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee14e7d5-700e-4d65-bf91-f6344539d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = pd.Series(data.index, \n",
    "                    index=data['Title']).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65709c09-2e02-44b6-a364-c4b258aa3df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3                          blackaf\n",
      "285                     washington\n",
      "417                 arrest develop\n",
      "434     astronomi club sketch show\n",
      "451    aunti donna big ol hous fun\n",
      "656                      big mouth\n",
      "752                bojack horseman\n",
      "805                   brew brother\n",
      "935                       champion\n",
      "937                   chappel show\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def netFlix_recommendation(title, similarity = similarity):\n",
    "    index = indices[title]\n",
    "    similarity_scores = list(enumerate(similarity[index]))\n",
    "    similarity_scores = sorted(similarity_scores, key=lambda x: x[1], reverse=True)\n",
    "    similarity_scores = similarity_scores[0:10]\n",
    "    movieindices = [i[0] for i in similarity_scores]\n",
    "    return data['Title'].iloc[movieindices]\n",
    "\n",
    "print(netFlix_recommendation(\"girlfriend\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2ace1-1bb3-429e-9f04-41c7de9b718e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
